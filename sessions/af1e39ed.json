{
  "messages": [
    {
      "role": "user",
      "content": "给我详细介绍以下内容，一定要详细准确：智能体将语言模型与工具结合，创建能够对任务进行推理、决定使用哪些工具并迭代寻求解决方案的系统。\n\ncreate_agent 提供了一个生产就绪的智能体实现。\n\nLLM 智能体在循环中运行工具以实现目标。\n智能体会一直运行，直到满足停止条件——即模型输出最终结果或达到迭代次数限制。\n\nmermaid-diagram\n信息\ncreate_agent 使用 LangGraph 构建了一个基于图的智能体运行时。图由节点（步骤）和边（连接）组成，定义了智能体如何处理信息。智能体在图中移动，执行节点，如模型节点（调用模型）、工具节点（执行工具）或中间件。\n\n了解更多关于 Graph API 的信息。\n\n核心组件\n模型\n模型 是智能体的推理引擎。它可以通过多种方式指定，支持静态和动态模型选择。\n\n静态模型\n静态模型在创建智能体时配置一次，并在整个执行过程中保持不变。这是最常见且直接的方法。\n\n从 模型标识符字符串 初始化静态模型：\n\n\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    \"openai:gpt-5\",\n    tools=tools\n)\n提示\n模型标识符字符串支持自动推断（例如 \"gpt-5\" 将被推断为 \"openai:gpt-5\"）。请参考参考文档 查看完整的模型标识符字符串映射列表。\n\n为了对模型配置进行更多控制，可以直接使用提供商包初始化模型实例。在此示例中，我们使用 ChatOpenAI。请参阅 聊天模型 以了解其他可用的聊天模型类。\n\n\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30\n    # ...（其他参数）\n)\nagent = create_agent(model, tools=tools)\n模型实例为您提供对配置的完全控制。当您需要设置特定参数，如 temperature、max_tokens、timeouts、base_url 和其他提供商特定设置时，请使用它们。请参考参考文档 查看模型上可用的参数和方法。\n\n动态模型\n动态模型在 运行时 根据当前 状态 和上下文进行选择。这支持复杂的路由逻辑和成本优化。\n\n要使用动态模型，请使用 @wrap_model_call 装饰器创建中间件，以修改请求中的模型：\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\n\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"根据对话复杂性选择模型。\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # 对较长的对话使用高级模型\n        model = advanced_model\n    else:\n        model = basic_model\n\n    request.model = model\n    return handler(request)\n\nagent = create_agent(\n    model=basic_model,  # 默认模型\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n警告\n在使用结构化输出时，不支持预绑定模型（已调用 bind_tools 的模型）。如果您需要使用结构化输出的动态模型选择，请确保传递给中间件的模型未预绑定。\n\n提示\n有关模型配置详细信息，请参阅 模型。有关动态模型选择模式，请参阅 中间件中的动态模型。\n\n工具\n工具赋予智能体执行行动的能力。智能体超越了简单的仅模型工具绑定，实现了：\n\n序列中的多个工具调用（由单个提示触发）\n适当的并行工具调用\n基于先前结果的动态工具选择\n工具重试逻辑和错误处理\n工具调用之间的状态持久化\n有关更多信息，请参阅 工具。\n\n定义工具\n将工具列表传递给智能体。\n\n\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"搜索信息。\"\"\"\n    return f\"结果：{query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"获取位置的天气信息。\"\"\"\n    return f\"{location} 的天气：晴朗，72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n如果提供空工具列表，智能体将仅包含一个 LLM 节点，不具备工具调用能力。\n\n工具错误处理\n要自定义工具错误的处理方式，请使用 @wrap_tool_call 装饰器创建中间件：\n\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain_core.messages import ToolMessage\n\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"使用自定义消息处理工具执行错误。\"\"\"\n    try:\n        return handler(request)\n    except Exception as e:\n        # 向模型返回自定义错误消息\n        return ToolMessage(\n            content=f\"工具错误：请检查您的输入并重试。({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n当工具失败时，智能体将返回带有自定义错误消息的 ToolMessage：\n\n\n[\n    ...\n    ToolMessage(\n        content=\"工具错误：请检查您的输入并重试。(division by zero)\",\n        tool_call_id=\"...\"\n    ),\n    ...\n]\nReAct 循环中的工具使用\n智能体遵循 ReAct（“推理 + 行动”）模式，在简短的推理步骤与针对性的工具调用之间交替，并将结果观察反馈到后续决策中，直到能够提供最终答案。\n\n提示：识别当前最受欢迎的无线耳机并验证其可用性。\n\n================================ 人类消息 =================================\n\n找出当前最受欢迎的无线耳机并检查其库存\n推理： “受欢迎程度具有时效性，我需要使用提供的搜索工具。”\n\n行动： 调用 search_products(\"wireless headphones\")\n\n\n================================== AI 消息 ==================================\n工具调用：\n  search_products (call_abc123)\n 调用 ID：call_abc123\n  参数：\n    query: wireless headphones\n\n================================= 工具消息 =================================\n\n找到 5 个匹配“wireless headphones”的产品。前 5 个结果：WH-1000XM5, ...\n推理： “在回答前，我需要确认排名第一的商品的可用性。”\n\n行动： 调用 check_inventory(\"WH-1000XM5\")\n\n\n================================== AI 消息 ==================================\n工具调用：\n  check_inventory (call_def456)\n 调用 ID：call_def456\n  参数：\n    product_id: WH-1000XM5\n\n================================= 工具消息 =================================\n\n产品 WH-1000XM5：库存 10 件\n推理： “我已经获取了最受欢迎的型号及其库存状态。现在可以回答用户的问题。”\n\n行动： 产生最终答案\n\n\n================================== AI 消息 ==================================\n\n我找到了无线耳机（型号 WH-1000XM5），库存 10 件...\n提示\n要了解更多关于工具的信息，请参阅 工具。\n\n系统提示\n您可以通过提供提示来塑造智能体处理任务的方式。system_prompt 参数可以作为字符串提供：\n\n\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"你是一个有帮助的助手。请简洁准确。\"\n)\n当未提供 system_prompt 时，智能体将直接从消息中推断其任务。\n\n动态系统提示\n对于需要根据运行时上下文或智能体状态修改系统提示的高级用例，您可以使用 中间件。\n\n@dynamic_prompt 装饰器创建中间件，根据模型请求动态生成系统提示：\n\n\nfrom typing import TypedDict\n\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"根据用户角色生成系统提示。\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"你是一个有帮助的助手。\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} 提供详细的技术响应。\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} 简单解释概念，避免使用行话。\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# 系统提示将根据上下文动态设置\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"解释机器学习\"}]},\n    context={\"user_role\": \"expert\"}\n)\n提示\n有关消息类型和格式化的更多详细信息，请参阅 消息。有关全面的中间件文档，请参阅 中间件。\n\n调用\n您可以通过向其 State 传递更新来调用智能体。所有智能体在其状态中包含消息序列；要调用智能体，请传递一条新消息：\n\n\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"旧金山天气如何？\"}]}\n)\n有关从智能体流式传输步骤和/或令牌的信息，请参阅流式传输指南。\n\n否则，智能体遵循 LangGraph Graph API 并支持所有相关方法。\n\n高级概念\n结构化输出\n在某些情况下，您可能希望智能体以特定格式返回输出。LangChain 通过 response_format 参数提供结构化输出策略。\n\nToolStrategy\nToolStrategy 使用人工工具调用生成结构化输出。这适用于任何支持工具调用的模型：\n\n\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"openai:gpt-4o-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"从以下内容提取联系信息：John Doe, john@example.com, (555) 123-4567\"}]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\nProviderStrategy\nProviderStrategy 使用模型提供商的原生结构化输出生成。这更可靠，但仅适用于支持原生结构化输出的提供商（例如 OpenAI）：\n\n\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n注意\n从 langchain 1.0 开始，不再支持直接传递模式（例如 response_format=ContactInfo）。您必须明确使用 ToolStrategy 或 ProviderStrategy。\n\n提示\n要了解结构化输出，请参阅 结构化输出。\n\n记忆\n智能体通过消息状态自动维护对话历史。您还可以配置智能体使用自定义状态模式，在对话期间记住额外信息。\n\n存储在状态中的信息可以被视为智能体的短期记忆：\n\n自定义状态模式必须扩展 AgentState 作为 TypedDict。\n\n定义自定义状态有两种方式：\n\n通过 中间件（推荐）\n通过 create_agent 上的 state_schema\n注意\n推荐通过中间件定义自定义状态，而不是通过 create_agent 上的 state_schema，因为它允许您将状态扩展概念上限定在相关中间件和工具的范围内。\n\nstate_schema 仍支持用于向后兼容，在 create_agent 上。\n\n通过中间件定义状态\n当您的自定义状态需要被特定中间件钩子和附加到该中间件的工具访问时，使用中间件定义自定义状态。\n\n\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# 智能体现在可以跟踪消息之外的额外状态\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"我更喜欢技术性解释\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n通过 state_schema 定义状态\n使用 state_schema 参数作为快捷方式，定义仅在工具中使用的自定义状态。\n\n\nfrom langchain.agents import AgentState\n\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n# 智能体现在可以跟踪消息之外的额外状态\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"我更喜欢技术性解释\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n注意\n从 langchain 1.0 开始，自定义状态模式必须是 TypedDict 类型。不再支持 Pydantic 模型和数据类。有关更多详细信息，请参阅 v1 迁移指南。\n\n提示\n要了解更多关于记忆的信息，请参阅 记忆。有关实现跨会话持久化的长期记忆的信息，请参阅 长期记忆。\n\n流式传输\n我们已经看到如何使用 invoke 调用智能体以获取最终响应。如果智能体执行多个步骤，这可能需要一段时间。为了显示中间进度，我们可以随着消息的发生而流式传回。\n\n\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"搜索 AI 新闻并总结发现\"}]\n}, stream_mode=\"values\"):\n    # 每个块包含该时间点的完整状态\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"智能体：{latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"正在调用工具：{[tc['name'] for tc in latest_message.tool_calls]}\")\n提示\n有关流式传输的更多详细信息，请参阅 流式传输。\n\n中间件\n中间件 为在执行的不同阶段自定义智能体行为提供了强大的扩展性。您可以使用中间件来：\n\n在调用模型之前处理状态（例如消息裁剪、上下文注入）\n修改或验证模型的响应（例如防护栏、内容过滤）\n使用自定义逻辑处理工具执行错误\n基于状态或上下文实现动态模型选择\n添加自定义日志、监控或分析\n中间件无缝集成到智能体的执行图中，允许您在关键点拦截和修改数据流，而无需更改核心智能体逻辑。\n\n提示\n有关全面的中间件文档，包括装饰器如 @before_model、@after_model 和 @wrap_tool_call，请参阅 中间件。"
    },
    {
      "role": "assistant",
      "content": "下面我把你给出的内容系统化、细化、串成一篇「从 0 到 1 理解 create_agent 智能体」的说明。会按概念 → 组件 → 高级特性来讲，所有代码和术语都围绕 LangChain 的 `create_agent` 与 LangGraph 智能体运行时。\n\n---\n\n## 一、智能体总体概念：模型 + 工具 + 推理循环\n\n**智能体（Agent）** 的核心思想是：  \n不仅让语言模型“回答问题”，而是让它在一个循环中：\n\n1. 阅读用户请求及当前状态\n2. 推理：下一步要干什么？\n3. 决定要不要调用工具（以及调用哪些、用什么参数）\n4. 执行工具，拿到结果\n5. 再次推理：是否已经足够回答？还是继续调用其他工具？\n6. 直到：\n   - 模型给出最终自然语言结果，或\n   - 达到迭代次数上限/其他停止条件\n\n**`create_agent`** 就是一个「开箱即用的生产级智能体实现」，它把上述逻辑封装好，你只需要提供：\n\n- 一个模型（LLM / Chat Model）\n- 一组工具（可选）\n- 一些配置（中间件、系统提示、结构化输出、状态模式等）\n\n---\n\n## 二、LangGraph：create_agent 背后的图运行时\n\n`create_agent` 底层是基于 **LangGraph Graph API** 搭建的智能体运行时。  \n\n### 1. 图的基本元素\n\n- **节点（Node）**：每个节点是一个步骤，比如：\n  - 模型节点：调用 LLM\n  - 工具节点：执行某个工具\n  - 中间件节点：在模型/工具调用前后处理数据\n- **边（Edge）**：表示节点之间的连接和执行流向，比如：\n  - 从模型节点 → 工具节点（当模型决定调用工具时）\n  - 从工具节点 → 模型节点（工具执行完，把结果返回给模型继续推理）\n\n智能体就是在这个图里“走路”：\n\n1. 从初始节点开始（通常是模型节点）\n2. 根据模型输出决定是否走向工具节点\n3. 工具完成后返回，再回到模型节点\n4. 直至进入“结束”节点（例如模型产出「最终答案」）\n\n### 2. 停止条件\n\n智能体循环直到：\n\n- 模型告诉系统“已经完成”（即输出最终回答），或\n- 达到配置的最大迭代次数（防止死循环）\n\n---\n\n## 三、模型（Model）：智能体的推理引擎\n\n模型是智能体做“思考”的大脑。\n\n`create_agent` 中的 **model** 可以是：\n\n- 静态模型：创建 agent 时就固定好，用到底\n- 动态模型：在运行时按状态/上下文选择不同模型\n\n### 3.1 静态模型\n\n**静态模型**最常见，也是默认使用方式。\n\n#### 方式一：直接使用模型标识符字符串\n\n```python\nfrom langchain.agents import create_agent\n\nagent = create_agent(\n    \"openai:gpt-5\",  # 或直接 \"gpt-5\"\n    tools=tools\n)\n```\n\n说明：\n\n- `\"gpt-5\"` 会自动推断为 `\"openai:gpt-5\"`（LangChain 内部有模型标识符映射）\n- 这是最简便的配置方法\n- 适用于不需要细粒度配置参数的场景\n\n#### 方式二：显式构造模型实例（推荐用于生产）\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=\"gpt-5\",\n    temperature=0.1,\n    max_tokens=1000,\n    timeout=30,\n    # 还可以设置 base_url、api_key、organization 等\n)\n\nagent = create_agent(model, tools=tools)\n```\n\n优点：\n\n- 完全掌控模型配置  \n- 可以设置温度、最大输出 token、请求超时、代理地址、base_url、API Key 等\n- 在不同环境（开发/生产）下可用不同参数\n\n> 参考文档中会列出 ChatOpenAI 等模型类支持的所有参数和方法。\n\n### 3.2 动态模型：运行时智能路由\n\n**动态模型**允许智能体在每次模型调用时，根据当前 **状态**和**上下文**来选择要用的模型。\n\n典型用途：\n\n- 根据对话长度/复杂度，简单问题用便宜模型，复杂问题用高级模型\n- 不同用户等级（免费/付费）使用不同模型\n- 根据任务类型（写代码/写文案）路由到不同模型\n\n实现方式是通过 **中间件** + `@wrap_model_call` 装饰器：\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n\nbasic_model = ChatOpenAI(model=\"gpt-4o-mini\")\nadvanced_model = ChatOpenAI(model=\"gpt-4o\")\n\n@wrap_model_call\ndef dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n    \"\"\"根据对话复杂性选择模型。\"\"\"\n    message_count = len(request.state[\"messages\"])\n\n    if message_count > 10:\n        # 对较长的对话使用高级模型\n        model = advanced_model\n    else:\n        model = basic_model\n\n    request.model = model   # 动态替换本次调用的模型\n    return handler(request) # 继续执行模型调用\n\nagent = create_agent(\n    model=basic_model,          # 默认模型\n    tools=tools,\n    middleware=[dynamic_model_selection]\n)\n```\n\n**注意 / 限制：**\n\n- 当你使用 **结构化输出** 时，不支持“预绑定模型”（即 `model.bind_tools(...)` 后再传给中间件）。\n- 如需要“结构化输出 + 动态模型选择”，传给中间件的必须是 **未预绑定** 的模型实例，由中间件负责设置工具/输出模式。\n\n---\n\n## 四、工具（Tools）：智能体的“手脚”\n\nLLM 本身只会“算字”，而工具让它能：\n\n- 查数据（搜索、数据库、API）\n- 调用业务系统（下单、改配置）\n- 执行代码/函数\n- 访问网络、文件系统等\n\n`create_agent` 在工具调用方面的能力远超简单的 `model.bind_tools(...)`：\n\n- 可以连续调用多个工具（在 ReAct 循环中）\n- 支持并行调用多个工具（当模型一次请求中返回多个 tool call）\n- 根据已有的工具结果动态选择下一个工具\n- 内建工具重试和错误处理逻辑\n- 在多个工具调用之间共享和持久化状态\n\n### 4.1 定义工具\n\n使用 `@tool` 装饰器，返回函数作为工具：\n\n```python\nfrom langchain.tools import tool\nfrom langchain.agents import create_agent\n\n@tool\ndef search(query: str) -> str:\n    \"\"\"搜索信息。\"\"\"\n    return f\"结果：{query}\"\n\n@tool\ndef get_weather(location: str) -> str:\n    \"\"\"获取位置的天气信息。\"\"\"\n    return f\"{location} 的天气：晴朗，72°F\"\n\nagent = create_agent(model, tools=[search, get_weather])\n```\n\n说明：\n\n- 函数签名中的参数类型（如 `query: str`）会被用来生成工具 schema，供模型调用\n- 函数 docstring 会作为工具描述告诉模型“这工具是干什么的”\n- `tools=[]`（空列表）时，智能体图中只有一个 LLM 节点，没有任何工具调用能力\n\n### 4.2 工具错误处理：自定义工具异常反馈\n\n使用 `@wrap_tool_call` 中间件，可统一处理工具调用错误，并给模型返回定制的错误说明：\n\n```python\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain_core.messages import ToolMessage\n\n@wrap_tool_call\ndef handle_tool_errors(request, handler):\n    \"\"\"使用自定义消息处理工具执行错误。\"\"\"\n    try:\n        return handler(request)   # 正常执行工具\n    except Exception as e:\n        # 返回给模型的不是异常，而是一条 ToolMessage\n        return ToolMessage(\n            content=f\"工具错误：请检查您的输入并重试。({str(e)})\",\n            tool_call_id=request.tool_call[\"id\"]\n        )\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    tools=[search, get_weather],\n    middleware=[handle_tool_errors]\n)\n```\n\n当工具报错时，智能体状态中的消息会出现类似：\n\n```python\nToolMessage(\n    content=\"工具错误：请检查您的输入并重试。(division by zero)\",\n    tool_call_id=\"...\"\n)\n```\n\n模型看到这条 ToolMessage 后，可以据此调整下一步动作（比如重新调用工具、更换参数等）。\n\n---\n\n## 五、ReAct 推理 + 行动循环中的工具使用\n\n智能体遵循 **ReAct（Reasoning + Acting）** 模式：  \n在一个循环中不断重复：\n\n1. **推理（思考）**：分析当前信息、任务目标、已有工具结果\n2. **行动（调用工具）**：选择一个或多个工具，构造参数并执行\n3. 把工具返回结果作为“观察（observation）”，送回模型继续推理\n\n你给的无线耳机例子说明了这个过程：\n\n> 用户提示：找出当前最受欢迎的无线耳机并检查其库存\n\n流程大致是：\n\n1. 模型判断：“受欢迎程度需要实时信息 → 必须使用搜索工具”\n2. 行动：调用 `search_products(\"wireless headphones\")`\n3. 工具返回：列表 + 头部几个产品\n4. 模型推理：“需要进一步确认第一名的库存”\n5. 行动：调用 `check_inventory(\"WH-1000XM5\")`\n6. 工具返回库存信息\n7. 模型推理：“信息齐全，可给出最终回答”\n8. 输出自然语言结果\n\nReAct 的好处：\n\n- 控制每次工具调用的目的与范围（更精确、成本更低）\n- 工具调用序列是模型自主规划的，而不是写死的流程图\n- 允许模型在工具返回不充分时继续调用其它工具或重试\n\n---\n\n## 六、系统提示（System Prompt）：给智能体定“人格和规则”\n\n`system_prompt` 可以用来设定智能体行为准则、角色、语气等：\n\n```python\nagent = create_agent(\n    model,\n    tools,\n    system_prompt=\"你是一个有帮助的助手。请简洁准确。\"\n)\n```\n\n- 若不提供 `system_prompt`，智能体会从对话消息中推断任务（更自由但不够可控）\n- 在生产中通常会显式设置 system prompt 来规范行为，例如：\n  - 回答要引用数据来源\n  - 不得执行危险操作\n  - 输出必须采用特定格式\n\n### 6.1 动态系统提示：根据上下文自动变化\n\n当系统提示需要根据运行时上下文（如用户角色、当前业务场景）动态变化时，使用中间件 + `@dynamic_prompt`：\n\n```python\nfrom typing import TypedDict\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\n\nclass Context(TypedDict):\n    user_role: str\n\n@dynamic_prompt\ndef user_role_prompt(request: ModelRequest) -> str:\n    \"\"\"根据用户角色生成系统提示。\"\"\"\n    user_role = request.runtime.context.get(\"user_role\", \"user\")\n    base_prompt = \"你是一个有帮助的助手。\"\n\n    if user_role == \"expert\":\n        return f\"{base_prompt} 提供详细的技术响应。\"\n    elif user_role == \"beginner\":\n        return f\"{base_prompt} 简单解释概念，避免使用行话。\"\n\n    return base_prompt\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    tools=[web_search],\n    middleware=[user_role_prompt],\n    context_schema=Context\n)\n\n# 调用时携带上下文\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"解释机器学习\"}]},\n    context={\"user_role\": \"expert\"}\n)\n```\n\n效果：\n\n- 同一个 agent，对新手（beginner）给出浅显解释，对专家（expert）给技术细节\n- 所有这种“角色差异”集中在动态 prompt 中间件中管理，代码结构清晰\n\n---\n\n## 七、调用 agent：invoke 与 LangGraph 状态\n\n智能体的状态（State）至少包含一条 **消息序列（messages）**。  \n要调用智能体，只需向其状态传入一条（或多条）新消息：\n\n```python\nresult = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"旧金山天气如何？\"}]}\n)\n```\n\n- `invoke` 会执行智能体内部的图（模型 + 工具 + 中间件）\n- 返回的是一个「最终状态」，例如包含所有消息、结构化输出等  \n- agent 还支持 LangGraph Graph API 的全部方法（如 `astream`, `abatch`, `update_state` 等）\n\n---\n\n## 八、结构化输出（Structured Output）\n\n当你需要模型输出严格按照某一数据结构（而不是自由文本）时使用 **结构化输出**。\n\n通过 `response_format` 参数指定策略，用两种策略中的一种：\n\n### 8.1 ToolStrategy：用工具调用来“装”结构化输出\n\n`ToolStrategy` 让模型通过“调用一个工具”来产生结构化数据，这种方式：\n\n- 对任何支持 tool calling 的模型都适用\n- 底层是“用一个工具 schema 约束模型输出”\n\n示例：\n\n```python\nfrom pydantic import BaseModel\nfrom langchain.agents import create_agent\nfrom langchain.agents.structured_output import ToolStrategy\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str\n\nagent = create_agent(\n    model=\"openai:gpt-4o-mini\",\n    tools=[search_tool],\n    response_format=ToolStrategy(ContactInfo)\n)\n\nresult = agent.invoke({\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"从以下内容提取联系信息：John Doe, john@example.com, (555) 123-4567\"\n    }]\n})\n\nresult[\"structured_response\"]\n# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n```\n\n### 8.2 ProviderStrategy：用模型厂商原生的结构化输出\n\n`ProviderStrategy` 利用模型提供商的原生 JSON / schema 功能：\n\n```python\nfrom langchain.agents.structured_output import ProviderStrategy\n\nagent = create_agent(\n    model=\"openai:gpt-4o\",\n    response_format=ProviderStrategy(ContactInfo)\n)\n```\n\n特点：\n\n- 更可靠（因为是模型厂商原生实现）\n- 仅适用于支持原生 structured output 的模型，例如 OpenAI 的某些模型\n\n**版本变化注意：**\n\n- 从 `langchain 1.0` 开始，不再支持直接 `response_format=ContactInfo` 这种写法。\n- 必须显式指定 `ToolStrategy(ContactInfo)` 或 `ProviderStrategy(ContactInfo)`。\n\n---\n\n## 九、记忆（Memory）：自定义状态与对话历史\n\n智能体在执行过程中，会持续维护一份 **AgentState**，其中必然包括 **消息列表（messages）**，这可视作短期记忆。\n\n你还可以自定义额外的状态字段，用来在对话中“记住”更多信息，比如用户偏好、当前任务 ID 等。\n\n**要求：**\n\n- 自定义 state schema 必须继承 `AgentState`，并是 **TypedDict**（从 v1 开始不再支持 Pydantic/数据类）\n\n有两种方法定义自定义状态：\n\n### 9.1 通过中间件定义状态（推荐）\n\n当额外状态只服务于某些中间件和工具时，推荐把状态定义写在中间件内部：\n\n```python\nfrom typing import Any\nfrom langchain.agents import AgentState\nfrom langchain.agents.middleware import AgentMiddleware\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nclass CustomMiddleware(AgentMiddleware):\n    state_schema = CustomState\n    tools = [tool1, tool2]\n\n    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n        # 可以在这里读写 state.user_preferences\n        ...\n\nagent = create_agent(\n    model,\n    tools=tools,\n    middleware=[CustomMiddleware()]\n)\n\n# 调用时传入额外状态\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"我更喜欢技术性解释\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n好处：  \n状态定义与使用它的中间件/工具放在一起，模块化、可维护。\n\n### 9.2 通过 create_agent 的 state_schema 定义状态（向后兼容）\n\n另一种方式是在 `create_agent` 上直接指定 `state_schema`：\n\n```python\nfrom langchain.agents import AgentState\n\nclass CustomState(AgentState):\n    user_preferences: dict\n\nagent = create_agent(\n    model,\n    tools=[tool1, tool2],\n    state_schema=CustomState\n)\n\nresult = agent.invoke({\n    \"messages\": [{\"role\": \"user\", \"content\": \"我更喜欢技术性解释\"}],\n    \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n})\n```\n\n注意：\n\n- 这种方式更“全局”，所有工具都能看到这个状态\n- 官方推荐：**新代码更建议用中间件方式**，state_schema 主要作为向后兼容存在\n\n---\n\n## 十、流式传输（Streaming）：边执行边展示过程\n\n如果智能体需要多轮工具调用，`invoke` 可能要等一段时间才能返回最终结果。  \n为增强用户体验，可以使用 `agent.stream(...)` 实时获取“中间状态”。\n\n示例：\n\n```python\nfor chunk in agent.stream({\n    \"messages\": [{\"role\": \"user\", \"content\": \"搜索 AI 新闻并总结发现\"}]\n}, stream_mode=\"values\"):\n    # 每个 chunk 是当前时刻的完整状态\n    latest_message = chunk[\"messages\"][-1]\n    if latest_message.content:\n        print(f\"智能体：{latest_message.content}\")\n    elif latest_message.tool_calls:\n        print(f\"正在调用工具：{[tc['name'] for tc in latest_message.tool_calls]}\")\n```\n\n说明：\n\n- `stream_mode=\"values\"`：直接给你状态字典（包括 messages、structured_response 等）\n- 你可以在前端把“正在调用工具 X”、“工具返回结果 Y”一条条展示出来，让用户看到智能体的思考和操作过程\n- 对于长流程、多工具调用的复杂任务非常有用\n\n---\n\n## 十一、中间件（Middleware）：深度定制智能体行为的关键\n\n**中间件**是 `create_agent` 扩展性最强的部分，你已经看到几个典型例子：\n\n- `@wrap_model_call`：动态模型选择\n- `@dynamic_prompt`：动态系统提示\n- `@wrap_tool_call`：工具错误处理\n\n还有更多钩子可以使用，例如：\n\n- `@before_model`：在每次调用模型前修改状态（裁剪历史、注入上下文等）\n- `@after_model`：在模型输出后检查或修改结果（敏感内容过滤、防护栏）\n- 自定义 `AgentMiddleware` 子类：同时组合多个钩子，配合自定义状态\n\n能做的事情包括：\n\n- 根据当前 state 调整模型参数（比如长对话自动调低温度）\n- 在模型调用前对消息进行“裁剪 / 摘要”，控制上下文长度\n- 对模型响应进行安全审查，不合规则重试或屏蔽\n- 对工具调用进行访问控制、审计、日志记录\n- 实现复杂的运行策略（例如限流、熔断、灰度发布不同模型版本）\n\n---\n\n## 总结：create_agent 能帮你做什么？\n\n把上面所有内容整合起来，`create_agent` 实际提供的是一个：\n\n1. 基于 LangGraph 图模型构建的智能体运行时\n2. 封装好的 ReAct 循环：推理 → 工具调用 → 观察 → 再推理\n3. 对模型选择（静态/动态）、工具调用、多步推理等的统一管理\n4. 支持：\n   - 动态系统提示\n   - 自定义状态（记忆）\n   - 结构化输出\n   - 流式传输\n   - 中间件扩展（模型前后、工具前后、错误处理等）\n\n你只需要：\n\n- 提供一个或一组模型（可以是静态或动态选择）\n- 提供工具函数（用 `@tool` 包装）\n- 按需添加中间件（动态 prompt、安全过滤、日志等）\n- 用 `agent.invoke` / `agent.stream` 等方法调用\n\n就可以搭建出可用于生产环境的、可扩展的智能体系统。\n\n如果你想，我可以在此基础上给出一个「完整可运行的最小 demo」，或者根据你的业务场景（比如电商、知识库问答、内部系统自动化）设计一个具体的 agent 架构示例。",
      "model": "gpt-5.1"
    },
    {
      "role": "user",
      "content": "你是谁"
    },
    {
      "role": "assistant",
      "content": "\n\n我是 Grok 4，由 xAI 构建的 AI 助手，旨在帮助你解答问题、提供见解，并以幽默和实用的方式互动。有什么我能帮你的？",
      "model": "grok-4"
    },
    {
      "role": "user",
      "content": "我上面说了什么内容"
    },
    {
      "role": "assistant",
      "content": "\n\n你上一个消息是：“你是谁”，我回答了我是 Grok 4，由 xAI 构建的 AI 助手。\n\n如果你指的是更早的消息，那是你提供的一个详细的中文文本，内容是关于 LangChain 中的“create_agent”函数及其相关概念的介绍，包括智能体的定义、LangGraph 图运行时、模型（静态和动态）、工具定义和错误处理、ReAct 循环、系统提示、动态提示、调用方法、结构化输出、记忆、流式传输以及中间件等。文本中包含代码示例、警告和提示，看起来像是一篇文档或教程的摘录，要求我“详细准确”地介绍这些内容。\n\n如果你想让我复述或总结具体部分，请告诉我更多细节！",
      "model": "grok-4"
    }
  ],
  "created_at": "2025-12-22T10:56:30.867124",
  "updated_at": "2025-12-22T11:44:53.827257",
  "title": "给我详细介绍以下内容"
}